{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nckHxo2X9omh"
   },
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsOZ64A79xxV"
   },
   "source": [
    "# Install detectron2: (Colab has CUDA 10.1 + torch 1.7)\n",
    "# See https://detectron2.readthedocs.io/tutorials/install.html for instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yk8r9tbi9v0w",
    "outputId": "3bc120f9-de6c-4ee6-e270-12616bb32501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1 True\n",
      "gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
      "Copyright (C) 2017 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyyaml==5.1\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "!gcc --version\n",
    "# import opencv as cv\n",
    "import torch\n",
    "# assert torch.__version__.startswith(\"1.7\")\n",
    "# !pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.7/index.html\n",
    "# exit(0)  # After installation, you need to \"restart runtime\" in Colab. This line can also restart runtimeK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSnnkzBj99Bh"
   },
   "source": [
    "# Setup detectron2 logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iWO7b-knuS5b",
    "outputId": "3b549921-6c71-48f9-a5be-311390a3cc5d"
   },
   "outputs": [],
   "source": [
    "import detectron2\n",
    "# from detectron2.utils.logger import setup_logger\n",
    "# setup_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6YqjyVL-AXP"
   },
   "source": [
    "# Import some common libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dISpi3JRuP0r"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-22fe296ae0e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "from google.colab.patches import cv2_imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VETFTisS-E-I"
   },
   "source": [
    "# Import some common detectron2 utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p99N-8kWTrMo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kAD9Dv2kuN3g"
   },
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHIUeuBHj4o7"
   },
   "source": [
    " # Mount your Google Drive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnSnXLdjrlcg"
   },
   "source": [
    "Run the code cell below to link your google drive to this document. It will create a link to permission requests and a verification code that needs entered after initializing the code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "StQ2jrO-Ckb1",
    "outputId": "ebdf6c4e-99f0-4bd9-f783-ca8cfd1128a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mo_cgwrVsEUH"
   },
   "source": [
    "After mounting the drive, look in the Colab file explorer (at the left side of the browser by default).The shared drive \"garden_imgs\" path should match the string below. **↓**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "d0KFFgM1rG-m"
   },
   "outputs": [],
   "source": [
    "# PATH_TO_IMGS = \"PATH TO garden_img\"\n",
    "PATH_TO_IMGS = \"/content/drive/Shareddrives/Weed_Detection\"  ##SHARED DRIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bimj-wq6qUPZ",
    "outputId": "a5950f36-0baa-4679-bc88-f09d85e10a81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_2.json  1_4.zip   1_7.json  1_9.zip\t2_3.json  2_5.zip   ipynb_checkpoints\n",
      "1_2.zip   1_5.json  1_7.zip   2_1.json\t2_3.zip   2_6.json  Testing_Data\n",
      "1_3.json  1_5.zip   1_8.json  2_1.zip\t2_4.json  2_6.zip\n",
      "1_3.zip   1_6.json  1_8.zip   2_2.json\t2_4.zip   2_7.zip\n",
      "1_4.json  1_6.zip   1_9.json  2_2.zip\t2_5.json  2_8.zip\n"
     ]
    }
   ],
   "source": [
    "!ls $PATH_TO_IMGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbZHXBhz82Mt"
   },
   "source": [
    "**↓** We want to unzip the zipped file we linked to in our previous line of code **↓**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7186_OL9lqoN",
    "outputId": "25eceae5-fcf5-4e0f-da4b-a0e4837b2dbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/drive/Shareddrives/Weed_Detection/Testing_Data/1_1.zip\n",
      "replace frame_000001.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!unzip \"/content/drive/Shareddrives/Weed_Detection/Testing_Data/1_1.zip\" \n",
    "!unzip \"/content/drive/Shareddrives/Weed_Detection/1_2.zip\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e9f3IGMXbDe"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIbAM2pv-urF"
   },
   "outputs": [],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "register_coco_instances(\"garden_training_set\", {}, \"/content/drive/Shareddrives/Weed_Detection/1_2.json\", \"/content/\")\n",
    "\n",
    "register_coco_instances(\"garden_testing_set\", {}, \"/content/drive/Shareddrives/Weed_Detection/Testing_Data/1_2.json\", \"/content/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ljbWTX0Wi8E"
   },
   "source": [
    "# Check that the data loaded correctly with visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_E1PJ1h0yTTX"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "dataset_dicts = DatasetCatalog.get(\"garden_training_set\")\n",
    "meta_data = MetadataCatalog.get(\"garden_training_set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HpwSuOv7n9vo"
   },
   "source": [
    "#Remove empty annotations from dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8KV-AB1YIKa"
   },
   "outputs": [],
   "source": [
    "dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rfRQXFYAqX0"
   },
   "outputs": [],
   "source": [
    "dataset_dicts = DatasetCatalog.get(\"garden_training_set\")\n",
    "for entry in dataset_dicts:\n",
    "  print(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXdIbiV1jdAv"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for d in random.sample(dataset_dicts, 3):\n",
    "    img = plt.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img, metadata=meta_data, scale=0.5)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlqXIXXhW8dA"
   },
   "source": [
    "## Train!\n",
    "\n",
    "Now, let's fine-tune a COCO-pretrained R50-FPN Mask R-CNN model on the balloon dataset. It takes ~6 minutes to train 300 iterations on Colab's K80 GPU, or ~2 minutes on a P100 GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7unkuuiqLdqd"
   },
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "cfg = get_cfg()\n",
    "# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"garden_training_set\",)\n",
    "# cfg.DATASETS.TEST = (\"garden_testing_set\")\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # pick a good Learning Rate\n",
    "cfg.SOLVER.MAX_ITER = 300    # iterations. 300 to make sure it runs before we train the full model.\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for starting out (default: 512 for full model)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2  # only has one class (weed). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfbiXatXO9xi"
   },
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "#Use the final weights generated after successful training for inference  \n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.8  # set the testing threshold for this model\n",
    "#Pass the validation dataset\n",
    "cfg.DATASETS.TEST = (\"boardetect_val\", )\n",
    "\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "dataset_dicts = get_board_dicts(\"Text_Detection_Dataset_COCO_Format/val\")\n",
    "for d in random.sample(dataset_dicts, 3):    \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=board_metadata, \n",
    "                   scale=0.8,\n",
    "                   instance_mode=ColorMode.IMAGE   \n",
    "    )\n",
    "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\")) #Passing the predictions to CPU from the GPU\n",
    "    cv2_imshow(v.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBXeH8UXFcqU"
   },
   "outputs": [],
   "source": [
    "# Look at training curves in tensorboard:\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zA6XGdts-ok0"
   },
   "source": [
    "## CODE BELOW IS CURRENTLY FROM DETECTRONS ORIGINAL DOCUMENT #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0e4vdDIOXyxF"
   },
   "source": [
    "## Inference & evaluation using the trained model\n",
    "Now, let's run inference with the trained model on the balloon validation dataset. First, let's create a predictor using the model we just trained:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ya5nEuMELeq8"
   },
   "outputs": [],
   "source": [
    "# Inference should use the config with parameters that are used in training\n",
    "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWq1XHfDWiXO"
   },
   "source": [
    "Then, we randomly select several samples to visualize the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5LhISJqWXgM"
   },
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import ColorMode\n",
    "dataset_dicts = get_balloon_dicts(\"balloon/val\")\n",
    "for d in random.sample(dataset_dicts, 3):    \n",
    "    im = cv2.imread(d[\"file_name\"])\n",
    "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
    "    v = Visualizer(im[:, :, ::-1],\n",
    "                   metadata=balloon_metadata, \n",
    "                   scale=0.5, \n",
    "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
    "    )\n",
    "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kblA1IyFvWbT"
   },
   "source": [
    "We can also evaluate its performance using AP metric implemented in COCO API.\n",
    "This gives an AP of ~70. Not bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9tECBQCvMv3"
   },
   "outputs": [],
   "source": [
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "evaluator = COCOEvaluator(\"balloon_val\", (\"bbox\", \"segm\"), False, output_dir=\"./output/\")\n",
    "val_loader = build_detection_test_loader(cfg, \"balloon_val\")\n",
    "print(inference_on_dataset(trainer.model, val_loader, evaluator))\n",
    "# another equivalent way to evaluate the model is to use `trainer.test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKBbjnLw5GGG"
   },
   "source": [
    "# Other types of builtin models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYJrlXZC5M-J"
   },
   "outputs": [],
   "source": [
    "# Inference with a keypoint detection model\n",
    "cfg = get_cfg()   # get a fresh new config\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "outputs = predictor(im)\n",
    "v = Visualizer(im[:,:,::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roTj1N9F5uJ5"
   },
   "outputs": [],
   "source": [
    "# Inference with a panoptic segmentation model\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\"))\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml\")\n",
    "predictor = DefaultPredictor(cfg)\n",
    "panoptic_seg, segments_info = predictor(im)[\"panoptic_seg\"]\n",
    "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "out = v.draw_panoptic_seg_predictions(panoptic_seg.to(\"cpu\"), segments_info)\n",
    "cv2_imshow(out.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiXadAb9Fv-L"
   },
   "source": [
    "# Run panoptic segmentation on a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YU5_W8wJF02F"
   },
   "outputs": [],
   "source": [
    "# This is the video we're going to process\n",
    "from IPython.display import YouTubeVideo, display\n",
    "video = YouTubeVideo(\"ll8TgCZ0plk\", width=500)\n",
    "display(video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a65jM_VFF2Hr"
   },
   "outputs": [],
   "source": [
    "# Install dependencies, download the video, and crop 5 seconds for processing\n",
    "!pip install youtube-dl\n",
    "!pip uninstall -y opencv-python-headless opencv-contrib-python\n",
    "!apt install python3-opencv  # the one pre-installed have some issues\n",
    "!youtube-dl https://www.youtube.com/watch?v=ll8TgCZ0plk -f 22 -o video.mp4\n",
    "!ffmpeg -i video.mp4 -t 00:00:06 -c:v copy video-clip.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyA4VmKcF61k"
   },
   "outputs": [],
   "source": [
    "# Run frame-by-frame inference demo on this video (takes 3-4 minutes) with the \"demo.py\" tool we provided in the repo.\n",
    "!git clone https://github.com/facebookresearch/detectron2\n",
    "!python detectron2/demo/demo.py --config-file detectron2/configs/COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml --video-input video-clip.mp4 --confidence-threshold 0.6 --output video-output.mkv \\\n",
    "  --opts MODEL.WEIGHTS detectron2://COCO-PanopticSegmentation/panoptic_fpn_R_101_3x/139514519/model_final_cafdb1.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OpLg_MAQGPUT"
   },
   "outputs": [],
   "source": [
    "# Download the results\n",
    "from google.colab import files\n",
    "files.download('video-output.mkv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "nckHxo2X9omh",
    "zA6XGdts-ok0",
    "0e4vdDIOXyxF",
    "oKBbjnLw5GGG",
    "hiXadAb9Fv-L"
   ],
   "name": "weed_detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
